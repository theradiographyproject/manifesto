# Introduction

The field of computer vision has achieved remarkable strides in the past decade, with its applications ranging from object detection in photographs to more complex tasks such as facial recognition and depth estimation. However, its application to medical imaging presents a distinct set of challenges and opportunities that differ significantly from traditional computer vision tasks. These differences stem from several unique characteristics inherent to medical imaging data.

The medical imaging domain as a whole presents unique challenges due to the sensitive and critical nature of the data involved. Factors such as limited data availability and scarcity of high-quality labels – primarily owing to patient privacy protections and the restricted number of qualified individuals able to annotate medical images – can have profound implications. These restrictions can compromise the accuracy of AI algorithms, potentially leading to subpar performance, decreased robustness during distribution shifts, and the propagation of biased outputs. Given the critical role of accurate diagnoses in medical care, these issues are of paramount importance and require concerted attention.

While some of these challenges are tangentially addressed by the techniques and topics discussed in this work, they are not the primary focus. Rather, this report aims to delve deeper into the intrinsic characteristics of medical imaging data itself, highlighting its distinction from traditional computer vision tasks. By examining the specific nature of medical imaging data and tasks, we aim to identify areas where alternative methodologies might prove beneficial or even necessary.

One of the core differences hypothesized is the absence of photographer bias, a concept referring to the influence of human intuition and knowledge that plays a role in capturing and framing images. In the realm of medical imaging, this photographer bias is absent, replaced by the clinical need to capture standardized views of the human anatomy across various imaging modalities, including MRI, CT, and PET scans. While this lack of bias ensures the consistency of the imaging data, it also removes the intuitive, human-guided structure that contrastive self supervised computer vision algorithms typically build upon when dealing with human-captured photographs.

Furthermore, the rich nature of medical imaging data necessitates the ability to work at various scales and understand diverse shapes. Diagnoses could range from identifying small, localized abnormalities requiring high-resolution scrutiny to discerning systemic changes that demand a broader, lower-resolution perspective. This demands a level of flexibility and scalability from algorithms that current computer vision models struggle to provide.

However, medical imaging also provides unique opportunities. The various imaging modalities and protocols often capture overlapping and redundant information, presenting an exciting potential for cross-modality learning. Additionally, each medical image is often accompanied by a detailed radiologist report, providing a valuable resource for further multi-modal learning.

Therefore, understanding the workflow and unique challenges faced by radiologists provides important insights for developing more robust and applicable machine learning models. This work will delve into these challenges, explore potential solutions, particularly in the context of transformer-based models, and outline a research proposal for tackling the pressing issues in the application of computer vision to medical imaging.

# The Role of Photographer Bias in Traditional Computer Vision and Its Absence in Medical Imaging

Photographer bias is a term referring to the influence and intuition of the individual capturing the image - the photographer. This concept plays a critical role in traditional computer vision tasks. A photograph is not merely a visual capture of reality; it is a rendition of the photographer's perception, incorporating their subjective decisions about composition, framing, focus, lighting, and scale. These decisions lend an implicit structure and hierarchy to the image, which serves as a strong prior for computer vision algorithms.

Photographer bias particularly aids in self-supervised contrastive learning, a prevalent approach for pretraining computer vision models. Contrastive learning operates by pushing augmented representations of the same or similar images together and pulling representations of dissimilar images apart in the learned embedding space. By leveraging the inherent structure in these captured photographs, these algorithms can identify "similar" and "dissimilar" images based on the inherent semantic concepts within them. This is because the photographer's intuition often centers on a specific object or scene, effectively providing a form of pre-segmentation and focus that algorithms can build upon.

However, this crucial element is absent in certain forms of medical imaging. Medical images, particularly cross sectional imaging, are captured systematically, not subjectively. The objective is not to create a visually appealing or balanced composition but to provide a standardized view of anatomy or pathology. These images often contain multiple independent semantic concepts that need to be understood and evaluated concurrently - a characteristic that traditional contrastive learning approaches may struggle to handle. In other words, while a natural image might focus on a single 'story', a medical image contains an anthology of 'stories' each requiring interpretation.

The absence of photographer bias in medical imaging draws an interesting parallel with augmented reality (AR). In AR, video is captured continuously with a wide field of view, encompassing everything in the scene without any deliberate framing or focusing. This raw, unfiltered, and unbiased representation of reality is more akin to the type of images seen in medical imaging, which are also captured without specific photographic intention. Therefore, advances in AR, particularly those that handle the challenges presented by the absence of photographer bias, can provide valuable insights for medical imaging research.

Contrastive learning, exemplified by methods such as SimCLR, BYOL, and DINO, can be further explored in [the following links]. In apparent contradiction to my argument above, these approaches have already achieved remarkable results in certain aspects of medical imaging, notably x-ray and pathology imaging. This might appear to contradict the previous argument regarding the inapplicability of photographer bias in the context of medical imaging. However, these successes can be considered exceptions that further validate the rule, given the unique characteristics of x-rays and pathology samples.

X-ray images inherently contain a projection of the entire depth of the body within a single field of view. This characteristic allows the pathology or 'story' to cover a larger proportion of the input space, making it more conducive to contrastive learning. Similarly, pathology slides benefit from an analogue to the 'photographer bias' - the 'biopsier bias'. This refers to the process of biopsy, where a specific, often symptomatic, part of the body is intentionally targeted for sampling and histologic imaging, thus creating a 'framed' snapshot of the body. However, such situations are less prevalent in cross-sectional imaging like CT or MRI scans.

# The Imperative of Handling Variable Resolution Scales and Shapes in Medical Imaging

Traditionally, computer vision research and development, especially those employing Convolutional Neural Networks (CNNs), has involved images with a uniform size and aspect ratio. This standardization removes the necessity to deal with variable aspect ratios and resolutions in the modeling process, simplifying the task. Furthermore, the introduction of photographer bias and human labeling also plays a role in minimizing resolution-scaling concerns. Photographers generally capture images such that the object of semantic importance is at an appropriate size/resolution within the image, reducing the need for resolution scaling.

However, such a strategy may not hold in the context of medical imaging. Objects of interest vary significantly in terms of their relative size and therefore different input sizes and resolutions are necessary. Hence, unless a different model is desired for every combination of input size/shape and resolution, algorithms for medical imaging must be flexible enough to handle these variations. While one may say that a model that can take in the entire scan all at once at a small patch size would be sufficient, modern compute resources are simply not powerful enough to allow this at either a training or inference level.

Vision Transformers (ViTs) present an opportunity to address this challenge. Unlike CNNs, ViTs can technically handle variable input sizes. This allows them to work with images of different aspect ratios. They also can scale their resolution by changing patch size. These characteristics make them potentially more suitable for medical imaging tasks. However, to date, this feature has not been extensively exploited. Most ViTs, in practice, still require a fixed image shape input. This is partly due to historical reasons – many image challenges that drove computer vision research featured images of fixed size. Another contributing factor is the use of learnable position embeddings. Although this technique doesn’t appear to be significantly more performant than absolute position embeddings, it became popular simply because it was the default option presented in the original ViT paper.

Nevertheless, there have been some notable exceptions. Google's [FlexiViT](https://arxiv.org/abs/2212.08013) allows for flexibility in resolutions by enabling variable patch sizes and this was extended further in their [Patch n' Pack paper](https://arxiv.org/abs/2307.06304). This system can train a model with a range of patch sizes, enabling the model to generalize to different resolutions at inference time without significant performance degradation. This approach can be particularly useful in medical imaging, where the detail level necessary to perceive certain pathologies can vary greatly.

Furthermore, Google's ["Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning"](https://arxiv.org/abs/2212.03229) offers a solution for flexible aspect ratios. By employing absolute position embeddings in three dimensions, this approach can handle voxel prisms of arbitrary aspect ratios. This strategy opens up the possibility of applying transformer reasoning to varied medical imaging tasks, each requiring different input shapes – be it imaging lungs, adrenal glands, or cancerous lesions.

# Leveraging Self-Supervised Learning Strategies in Medical Imaging

While contrastive learning has proven powerful in traditional computer vision tasks, its utility in medical imaging, for the reasons described above may be more limited. However, other self-supervised learning strategies, particularly those involving self-prediction and exploitation of innate relationships, offer a promising path forward. To maximize the effectiveness of these approaches, we propose anchoring them with a radiology trainee's natural learning progression. This journey involves 1) mastering the understanding of normal anatomy, 2) developing skills to describe and cross-correlate findings with other modalities and reports, and 3) finally synthesizing these findings into a comprehensive diagnostic impression. The third step likely requires the development of artificial general intelligence (AGI) but the first two steps may be more tractable and we will explore them further.

## Part 1: Mastering Normal Anatomy
The first step in a radiology trainee's journey is learning to recognize normal anatomy, which provides a basis for identifying anomalies. One possible approach to leverage the innate structure of human anatomy for self-supervision. Namely, the insight being exploited is that human anatomy demonstrates a pattern of spatial relationships between various anatomic entities – head comes before the shoulders comes before the knees comes before the toes. One way that we propose to do this is the idea of voxel prism ordering where two subsets of voxels from a scan are sampled and the model predicts the spatial relationship between them. This idea is conceptually similar to the work of Misra's ShuffleToLearn, where the algorithm learns to identify the correct order for a triplet of shuffled frames drawn from a video. In the context of medical imaging, an algorithm could learn to identify the correct spatial relationship of voxel prisms, thereby learning the typical structure of human anatomy.

## Part 2: Describing and Cross-Correlating Findings
After acquiring a solid understanding of normal anatomy, a trainee radiologist moves on to describing pathological findings and correlating them across different imaging modalities and protocols. This can be approached from multiple angles, leveraging self-prediction and cross-modality learning tasks.

**Describing Findings in Pixel Space:** Here, a masked autoencoder, inspired by Meta's approach, can be employed. This technique involves the prediction of masked or omitted sections of an image, analogous to the method employed in GPT models, but extended to the image domain. It allows the model to learn semantic structures within the images, thereby facilitating the description of findings.

It is noteworthy that both this method and the voxel prism ordering approach can be applied to individual image series, eliminating the need for extraneous data beyond the series of frames comprising a scan. This ensures remarkable flexibility in utilizing the considerable volume of imaging data publicly available for research and development. This strategy presents an opportunity to address one of the current limitations in medical imaging: developing and disseminating public algorithms based on publicly available datasets. These algorithms can be executed and validated by independent researchers, further contributing to the collective knowledge in the field. The subsequent two approaches pose more complexity due to their dependency on the creation of datasets with particular structure, necessitating greater institutional coordination.

**Cross-Correlating with Other Modalities:** An intriguing form of self-supervision that becomes viable with some of the above described technical changes is multimodality imaging. Commonly, the same anatomical area is imaged multiple times within a short timeframe using various modalities and protocols. The rationale behind this lies in the fact that certain modalities and protocols are necessary to provide definitive answers to different clinical inquiries.

For instance, a CT scan might raise suspicion for a specific condition, necessitating an MRI for confirmation. This process leads to the generation of two medical scans taken close together in time. While it is impossible to recreate a complete MRI from a CT scan (or vice versa) - a notion that would defy the need for an MRI in the first place - some degree of redundancy exists between the two images that a model can exploit as a learning strategy. Predicting an entire CT or MRI scan from the other would be computationally intensive. However, if a model equipped with an anatomy recognizer, trained using voxel ordering, is used, voxel prism subsets between the CT and MRI can be matched anatomically, allowing for prediction between the two at computationally manageable levels. This approach extends beyond modalities and can incorporate protocols and sequences, enabling cross training between CT, MR, PET, and varying protocols such as contrast/no contrast, FLAIR versus T1, and different radiotracers.

While this technique is largely unique to medical imaging, Meta's ImageBind also employs a similar concept by capturing audio, depth maps, and heat maps of the same scenes. Although medical imaging may be distinct from traditional photography, it shares considerable similarities with the challenges of augmented reality. In both scenarios, an unfiltered, comprehensive view of the surroundings is presented, rather than human-selected snapshots. Consequently, it is not surprising that Meta's research frequently aligns with medical imaging applications.

**Describing Findings with Words:** The final phase of this investigation, which ironically tends to be the primary focal point for many computer scientists examining medical imaging, pertains to the radiology report. Initially, many thought that an extensive dataset of medical images paired with their corresponding reports would make the development of a model capable of substituting radiologists trivial. This expectation was largely driven by a general lack of understanding within the computer science community about the role of radiologists, especially the extent to which the job encompasses areas beyond the images themselves. However, a significant component of the struggle also involves some of the technical challenges discussed above.

To construct a report, continuous magnification and reduction of scans, the simultaneous examination of multiple modalities, referencing to established anatomical knowledge, and correlating findings with the medical chart are all integral. When computational limitations prevent a model from even being able to accommodate varying aspect ratios and resolutions, predicting a comprehensive radiology report becomes a formidable challenge.

Nevertheless, building on the groundwork laid by the previously outlined steps, it may be plausible to deconstruct reports into relevant image contexts for a multimodal image-to-text comparison. For instance, one could adopt a specific resolution for examining the adrenal gland and associate that with the part of the report addressing the same. A different resolution and image size could be employed for another section. Often, different parts of the report are independent of each other, making it feasible to match descriptions in a way that promotes stable training.

However, I maintain that bypassing the earlier steps and progressing directly to the image-to-report stage would only be viable given a truly massive dataset and extensive computational resources, necessitating full-scan image sizes with exceedingly small patch sizes. Consequently, it is not surprising that image-to-report work has predominantly found success with x-rays, where such an approach is computationally feasible. By integrating these self-supervised learning strategies into a flexible transformer model tailored for medical imaging, we may be able to establish a more performant baseline to develop medical imaging algorithms on top of.

# Conclusion: The Bigger Picture
In this discussion, we've skirted around a term that's become quite popular since the advent of ChatGPT - foundation models. Essentially, we've been piecing together a hypothesis for a blueprint to create such models specifically tailored for medical imaging. Foundation models offer a myriad of intriguing possibilities, not least of which is their potential to mitigate some of the problems highlighted earlier in this discussion. Medical supervised models, often trained on limited datasets, are extremely fragile when confronted with shifts in distribution. They might also suffer from accuracy issues, which can pose severe risks in a medical context. A foundation model, although not a silver bullet for these concerns (we know current Language Model (LM) foundation models have their own issues with bias and inappropriate behavior), could enhance the baseline performance. This small but crucial step could lead to more reliable results, requiring rigorous testing, ongoing validation, and vigilant supervision of these models. Beyond that, a foundation model could potentially streamline label generation by creating candidate segmentations that radiologists can swiftly verify, saving them from the laborious task of creating labels from scratch. This could encourage innovative applications of AI in medical imaging that we might not have yet contemplated.

While the deployment of medical AI algorithms isn't widespread yet, their usage is steadily increasing. This rise is likely to continue, leading to a growing demand for computational resources for inference. While inference usually demands fewer computational resources, a large number of models could introduce substantial latency into a medical system, where time-sensitive decisions are the norm. An intriguing opportunity presented by visual foundation models, as exemplified by Meta's 'segment anything' model, is the potential performance improvements through precomputed embeddings. These embeddings can be utilized to construct downstream models, resulting in models significantly smaller than the original. In the case of Meta's model, there's a tenfold reduction in inference time for the downstream model compared to the embedding model, despite the former running on a client's CPU and the latter on a GPU server. This architecture could allow for an exponential increase in the number of algorithms without a corresponding exponential increase in the computational resources required, making for a more efficient system overall.

